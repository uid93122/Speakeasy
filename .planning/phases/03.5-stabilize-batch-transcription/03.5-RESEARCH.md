# Phase 03.5: Stabilize Batch Transcription - Research

**Researched:** 2026-01-30
**Domain:** Error Handling & Concurrency
**Confidence:** HIGH

## Summary

This phase addresses critical stability issues in batch transcription, specifically `CUDA error: an illegal memory access` and `no running event loop` crashes. The research identifies that "illegal memory access" typically corrupts the CUDA context, requiring aggressive recovery strategies (model reload) or process isolation. The "event loop" error is a threading violation where async tasks are spawned from background threads without referencing the main event loop.

**Primary recommendation:** Implement a robust "Catch, Unload, Reload" cycle for CUDA errors in the batch loop, and use `asyncio.run_coroutine_threadsafe` for all state change broadcasts.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| `faster-whisper` | 1.0+ | Transcription Engine | Current engine, relies on CTranslate2 |
| `asyncio` | 3.10+ | Concurrency | Python standard for async I/O |
| `torch` | 2.x | GPU Management | Used for VRAM management and cleaning |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| `sounddevice` | 0.4+ | Audio I/O | Captures audio (threading source) |

## Architecture Patterns

### Pattern 1: Thread-Safe Async Broadcasting
**What:** Broadcasting WebSocket messages from background threads (e.g., audio callbacks or worker threads) back to the main `asyncio` loop.
**When to use:** Anytime a non-async thread (worker/callback) needs to trigger a UI update.
**Example:**
```python
# Source: Python asyncio documentation
def on_state_change_safe(state: TranscriberState):
    """Thread-safe state update."""
    loop = get_main_event_loop() # Captured at startup
    if loop and loop.is_running():
        asyncio.run_coroutine_threadsafe(broadcast_coro(state), loop)
```

### Pattern 2: CUDA "Soft" Restart
**What:** recovering from CUDA errors by destroying the model instance and forcing garbage collection, attempting to clear the context without killing the process.
**When to use:** When a batch job fails with a GPU error but we want to attempt to save the rest of the batch.
**Example:**
```python
try:
    transcribe_file(...)
except RuntimeError as e:
    if "CUDA" in str(e) or "illegal memory access" in str(e):
        logger.error("CUDA Error detected. Reloading model...")
        transcriber.unload_model() # calls gc.collect() and empty_cache()
        transcriber.load_model(...) # Re-initializes CTranslate2 context
```

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| **Thread-Async Bridge** | Custom queues/polling | `asyncio.run_coroutine_threadsafe` | Built-in, handles race conditions correctly |
| **CUDA Cleanup** | Manual pointer deletion | `gc.collect()` + `torch.cuda.empty_cache()` | Python handles obj lifecycle, Torch handles caching |

## Common Pitfalls

### Pitfall 1: "No running event loop"
**What goes wrong:** Calling `asyncio.create_task()` from a thread created by `asyncio.to_thread` or `threading.Thread`.
**Why it happens:** Worker threads do not have an active asyncio event loop attached by default.
**How to avoid:** Always reference the *main* application loop when scheduling tasks from workers.
**Warning signs:** `RuntimeError: no running event loop` in logs during batch processing or recording stop.

### Pitfall 2: Optimistic CUDA Recovery
**What goes wrong:** Assuming `torch.cuda.empty_cache()` fixes "illegal memory access".
**Why it happens:** Illegal access often corrupts the underlying CUDA context state, not just memory allocation.
**How to avoid:** Assume the model instance is "poisoned". You must fully unload/destroy the model object and create a fresh one.
**Risk:** If the context corruption is process-wide, even this might fail (requiring process restart), but it's the best in-process attempt.

## Code Examples

### Robust Batch Processing Loop
```python
# backend/speakeasy/services/batch.py

# ... inside process_job loop ...
try:
    result = await asyncio.to_thread(transcriber.transcribe_file, ...)
    # success logic
except Exception as e:
    error_msg = str(e)
    if "CUDA" in error_msg or "illegal memory access" in error_msg:
        logger.critical(f"GPU Crash on file {bf.filename}. Attempting model reload.")
        
        # 1. Unload poisoned model
        await asyncio.to_thread(transcriber.unload_model)
        
        # 2. Wait briefly
        await asyncio.sleep(1)
        
        # 3. Reload (using saved settings)
        # Note: You need access to the settings to know what to reload
        await asyncio.to_thread(transcriber.load_model, ...)
        
        bf.error = "GPU Error - Model Reloaded"
    else:
        bf.error = error_msg
    
    bf.status = BatchFileStatus.FAILED
    failed_count += 1
```

### Thread-Safe Broadcast (Server Fix)
```python
# backend/speakeasy/server.py

# Global to hold main loop
main_event_loop: Optional[asyncio.AbstractEventLoop] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global main_event_loop
    main_event_loop = asyncio.get_running_loop()
    # ... initialization ...
    yield
    # ... cleanup ...

def on_state_change(state: TranscriberState) -> None:
    """Handle state changes thread-safely."""
    if main_event_loop and main_event_loop.is_running():
        asyncio.run_coroutine_threadsafe(
            broadcast("status", {"state": state.value, ...}),
            main_event_loop
        )
    else:
        # Fallback if loop isn't ready (shouldn't happen in normal op)
        logger.warning("Main loop not available for broadcast")
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| `asyncio.create_task` everywhere | `run_coroutine_threadsafe` for workers | Phase 03.5 | Fixes random crashes during batching |
| Ignored CUDA errors | Catch & Reload Model | Phase 03.5 | Batch continues after one bad file |
| High Beam Size (5) | Low Beam Size (1) for Batch | Phase 03.5 | Reduces VRAM usage and OOM risk |

## Open Questions

1.  **Process-Wide Corruption**
    *   **Question:** Does CTranslate2 "illegal memory access" corrupt the entire process CUDA context or just the `WhisperModel` instance?
    *   **Hypothesis:** Likely just the instance if CTranslate2 manages contexts well, but PyTorch interaction might complicate this.
    *   **Recommendation:** Implement the "Soft Restart". If crashes persist, Phase 04 should investigate `multiprocessing` isolation.

## Sources

### Primary (HIGH confidence)
- **Python Asyncio Docs**: Thread-safe scheduling patterns.
- **CTranslate2/Faster-Whisper Issues**: Common OOM and error patterns.
- **Project Codebase**: `server.py` and `BatchService` logic analysis.

### Secondary (MEDIUM confidence)
- **StackOverflow/GitHub Discussions**: CUDA error recovery strategies in Python.

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Core libs are well defined.
- Architecture: HIGH - Thread-safety pattern is standard.
- Pitfalls: HIGH - Diagnosis of event loop error is definitive.

**Research date:** 2026-01-30
**Valid until:** 2026-04-30
