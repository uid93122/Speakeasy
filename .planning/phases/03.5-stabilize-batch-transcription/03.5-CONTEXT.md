# Phase 03.5: Stabilize Batch Transcription - Context

**Gathered:** 2026-01-30
**Status:** Ready for planning

<domain>
## Phase Boundary

Fix the critical `CUDA error: an illegal memory access` crashes during batch transcription. Ensure the system is robust against GPU failures and handles concurrency correctly without leaking resources or crashing the event loop.

</domain>

<decisions>
## Implementation Decisions

### CUDA Error Handling
- **Isolate & Restart:** If a CUDA error occurs, the system must detect it, mark the current file as failed, and crucially **re-initialize the model/context** so subsequent files can process. The batch should not be abandoned.
- **Error Reporting:** Explicitly report "GPU Error" to the user, distinguishing it from generic file errors.

### Concurrency
- **Sequential Processing:** Batch files must be processed **sequentially** (one at a time) to prevent VRAM exhaustion and race conditions, given the heavy nature of the transcription model.
- **Queue Management:** Ensure the batch queue is robust and doesn't get stuck if one item crashes the worker.

### Claude's Discretion
- **Technical Fix Details:** Exact method for clearing CUDA cache/context (e.g., `torch.cuda.empty_cache()`, process restart, or model reload).
- **Event Loop Fix:** Resolving the "no running event loop" error which indicates async/threading misuse.

</decisions>

<specifics>
## Specific Ideas

- "Crash entire batch vs isolate file vs restart model? -> Restart model/context to save the batch."
- "Sequential processing to ensure stability."

</specifics>

<deferred>
## Deferred Ideas

None â€” discussion stayed within phase scope.

</deferred>

---

*Phase: 03.5-stabilize-batch-transcription*
*Context gathered: 2026-01-30*
